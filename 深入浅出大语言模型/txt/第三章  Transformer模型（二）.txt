第三章 Transformer模型（二）


3.2 注意力机制

认知心理学

q 选择性注意现象

3.2 注意力机制

词的语义由其所在上下文中与其相关的词决定

实现词向量的动态表示

解决长距离依赖问题

“一回到家我就吃了一个非常美味的苹果”

“一回到家我就用苹果跟朋友联系”

水果

手机

3.2 注意力机制

注意力机制

q 借用心理学的概念，按照重要性程度的不同，有选择地获取信息，用于

概念的组合表示。

“一回到家我就吃了一个非常美味的苹果”

“一回到家我就用苹果跟朋友联系”

水果

手机

一个例子：中年人的平均收入

序号 姓名 年龄 年收入（万）

1 张三 45 30

2 李四 28 40

3 王五 50 45

4 马六 57 35

5 黄七 39 20

6 钱八 43 33

7 刘九 60 42

8 孙十 47 50

中年人

q 40岁-55岁

一个例子：中年人的平均收入

模糊求解

q 不同年龄按照与“中年人”的相似度做加权处理

查询、键、值问题

q 按照查询与键的相似度计算的加权平均值

查询：中年人

键：年龄

值：收入

相似度越大越被注意——注意力机制

 

?

 归一化相似度 中年人, 年龄?) ∙ 收入?)

注意力机制

设查询q，键?? ? = 1, …, ?)，值为?? ? = 1, …, ?)

q、??、??

??? ?, ?

均为向量

?)表示查询q与键??的相似度

? ?, ??)表示查询q与键??的归一化相似度

 

?=1

?

? ?, ??) = 1

? ?, ??) = softmax ??? ?, ??) =

?

??? ?, ??)

 ?=1

? ?

??? ?, ?? 

注意力机制

注意力机制的计算

q 以归一化相似度为权重的加权和：

q ? ?, ??)又称作注意力权重

相似性计算：

除以维度d的开方防止维度比较大时相似度过大

? = 

?=1

?

? ?, ??)??

??? ?, ??) =

??

?

?

?

注意力机制

查询向量q

q 维度为d的行向量

键向量?? ? = 1,2, …, ?)

q m个维度为d的行向量

值向量?? ? = 1,2, …, ?)

q m个维度为d的行向量

键矩阵与值矩阵

? = ?

?

?

?

⋮

1

2 ? = ?

?

?

?

⋮

1

2 

注意力机制

注意力机制的矩阵表示

q 结果为维度为d的行向量

𝑎? ?,?, ?) = 𝑠𝑎??? 

??

?

? ∙ ?

注意力机制

设n个维度为d的查询?? ? = 1,2, …, ?)组成查询矩阵

注意力矩阵（ n行d列）：

𝑎? ?,?, ?) =

 

 

 

 

 

 

 

 

𝑠𝑎??? 

?1?

?

? ∙ ?

𝑠𝑎??? 

?2?

?

? ∙ ?

⋮

𝑠𝑎??? 

???

?

? ∙ ?

 

 

 

 

 

 

 

 

= 𝑠𝑎??? ??

?

? ∙ ?

? = ?

?

?

⋮

?

1

2 

3.3 自注意力机制

通过自注意力机制实现词向量的动态表示

设长度为n的输入序列?为?1, ?2, …, ??

q 其中??是序列中第i个位置单词的词向量，维度为d。

将??当作查询q

序列中的每个??都当作是键??

键??的值??就是??自己

这种计算方法称作自注意力机制

q 查询矩阵Q、键矩阵K和值矩阵V均为X，

q 计算序列自己与自己的注意力

体现了序列中第i个单词??在该上下文中的语义信息

3.3 自注意力机制

以??作为矩阵的第i行，则输入序列可以用n行d列的矩阵X表示：

自注意力矩阵：

? = ?

?

?

⋮

?

1

2 

𝑎? ?,?, ?) = 𝑎? ?, ?, ?)

= 𝑠𝑎??? ??

?

? ∙ ?

